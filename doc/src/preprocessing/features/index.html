<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.preprocessing.features API documentation</title>
<meta name="description" content="Auxiliary functions to compute features during preprocessing." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.preprocessing.features</code></h1>
</header>
<section id="section-intro">
<p>Auxiliary functions to compute features during preprocessing.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Auxiliary functions to compute features during preprocessing.&#34;&#34;&#34;


from preprocessing.features.embeddings import create_embed_feat
from preprocessing.features.gltr import create_gltr_feat
from preprocessing.features.polynomial import create_poly_feat
from preprocessing.features.manual_regex import create_regex_feat
from preprocessing.features.pos_tagging import create_tagging_feat
from preprocessing.features.tfidf import create_idf_feat


__all__ = [
    &#34;create_embed_feat&#34;,
    &#34;create_gltr_feat&#34;,
    &#34;create_poly_feat&#34;,
    &#34;create_regex_feat&#34;,
    &#34;create_tagging_feat&#34;,
    &#34;create_idf_feat&#34;
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="src.preprocessing.features.embeddings" href="embeddings.html">src.preprocessing.features.embeddings</a></code></dt>
<dd>
<div class="desc"><p>Compute embeddings features.</p></div>
</dd>
<dt><code class="name"><a title="src.preprocessing.features.gltr" href="gltr.html">src.preprocessing.features.gltr</a></code></dt>
<dd>
<div class="desc"><p>Statistical detection of human-written or GPT-2/BERT generated texts …</p></div>
</dd>
<dt><code class="name"><a title="src.preprocessing.features.manual_regex" href="manual_regex.html">src.preprocessing.features.manual_regex</a></code></dt>
<dd>
<div class="desc"><p>Compute features based on regex expressions.</p></div>
</dd>
<dt><code class="name"><a title="src.preprocessing.features.polynomial" href="polynomial.html">src.preprocessing.features.polynomial</a></code></dt>
<dd>
<div class="desc"><p>Compute polynomial features by using the <code>scikit-learn</code> API.</p></div>
</dd>
<dt><code class="name"><a title="src.preprocessing.features.pos_tagging" href="pos_tagging.html">src.preprocessing.features.pos_tagging</a></code></dt>
<dd>
<div class="desc"><p>Compute features based on Part-of-Speech tagging.</p></div>
</dd>
<dt><code class="name"><a title="src.preprocessing.features.tfidf" href="tfidf.html">src.preprocessing.features.tfidf</a></code></dt>
<dd>
<div class="desc"><p>Compute features related to tf-idf.</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.preprocessing.features.create_embed_feat"><code class="name flex">
<span>def <span class="ident">create_embed_feat</span></span>(<span>train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, embed_feat: Optional[List[str]] = None, more_docs: Optional[pandas.core.frame.DataFrame] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create embeddings features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>embed_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all embed features to compute. The name of a embed feature should be of the form
"A". A is the name of an embedding from "glove" and "google", or if any other name, it
will be trained.</dd>
<dt><strong><code>more_docs</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Additional dataframe containing a "document" column to use for idf computation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_embed_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    embed_feat: Optional[List[str]] = None, more_docs: Optional[DataFrame] = None,
):
    &#34;&#34;&#34;Auxiliary function to create embeddings features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    test_df : optional DataFrame, default=None
        Test dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    embed_feat : optional list of str, default=None
        List of all embed features to compute. The name of a embed feature should be of the form
        &#34;A&#34;. A is the name of an embedding from &#34;glove&#34; and &#34;google&#34;, or if any other name, it
        will be trained.

    more_docs : optional DataFrame, default=None
        Additional dataframe containing a &#34;document&#34; column to use for idf computation.
    &#34;&#34;&#34;
    if embed_feat is None or len(embed_feat) == 0:
        return

    print(&#34;\nComputing embeddings features...&#34;)

    init_feat = set(train_df.columns)
    new_feat = set()

    # Tokenization parameters
    stopwords = nltk.corpus.stopwords.words(&#34;english&#34;)
    punct = string.punctuation.replace(&#34;-&#34;, &#34;&#34;)

    for feat_name in embed_feat:

        embeddings_index = load_embeddings(
            train_df, more_docs=more_docs, embeddings_name=feat_name
        )

        # pylint: disable=cell-var-from-loop
        # Create embeddings vectors
        train_df[&#34;summary_&#34; + feat_name] = train_df[&#34;summary&#34;].progress_apply(
            lambda x: text_to_vec(embeddings_index, x, stopwords, punct, remove_stopwords=True)
        )
        if test_df is not None:
            test_df[&#34;summary_&#34; + feat_name] = test_df[&#34;summary&#34;].progress_apply(
                lambda x: text_to_vec(embeddings_index, x, stopwords, punct, remove_stopwords=True)
            )

        embed_cols = [f&#34;summary_{feat_name}_{i}&#34; for i in range(300)]

        train_df[embed_cols] = pd.DataFrame(
            train_df[&#34;summary_&#34; + feat_name].tolist(), index=train_df.index
        )
        if test_df is not None:
            test_df[embed_cols] = pd.DataFrame(
                test_df[&#34;summary_&#34; + feat_name].tolist(), index=test_df.index
            )

        new_feat = new_feat.union(embed_cols)

    # Drop intermediary features
    inter_feat = set(train_df.columns).difference(init_feat.union(new_feat))

    train_df.drop(columns=inter_feat, inplace=True)
    if test_df is not None:
        test_df.drop(columns=inter_feat, inplace=True)

    print(f&#34;Number of embeddings features: {len(new_feat)}.&#34;)</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.create_gltr_feat"><code class="name flex">
<span>def <span class="ident">create_gltr_feat</span></span>(<span>train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, gltr_feat: Optional[List[str]] = None) ‑> Tuple[pandas.core.frame.DataFrame, Optional[pandas.core.frame.DataFrame]]</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create GLTR features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing a "summary" column.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing a "summary" column.</dd>
<dt><strong><code>gltr_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all GLTR features to compute. The name of a GLTR feature should be of the form
"A_B". A is the name of a topk value computed by GLTR, it can be "count" or "frac". B is
the number of bins to compute the feature.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If one of the features is not supported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_gltr_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    gltr_feat: Optional[List[str]] = None
) -&gt; Tuple[DataFrame, Optional[DataFrame]]:
    &#34;&#34;&#34;Auxiliary function to create GLTR features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing a &#34;summary&#34; column.

    test_df : optional DataFrame, default=None
        Test dataframe containing a &#34;summary&#34; column.

    gltr_feat : optional list of str, default=None
        List of all GLTR features to compute. The name of a GLTR feature should be of the form
        &#34;A_B&#34;. A is the name of a topk value computed by GLTR, it can be &#34;count&#34; or &#34;frac&#34;. B is
        the number of bins to compute the feature.

    Raises
    ------
    ValueError
        If one of the features is not supported.
    &#34;&#34;&#34;
    if gltr_feat is None or len(gltr_feat) == 0:
        return train_df, test_df

    print(&#34;\nComputing GLTR features...&#34;)

    init_feat = set(train_df.columns)

    dfs = [train_df]
    if test_df is not None:
        dfs.append(test_df)

    # Features parameters
    gltr_bins = {}

    for feat_name in gltr_feat:

        feat_split = feat_name.split(&#34;_&#34;)
        feat_type = feat_split[0]
        feat_bins = int(feat_split[1])

        if feat_type not in {&#34;count&#34;, &#34;frac&#34;}:

            err_msg = f&#34;From GLTR feature {feat_name}, unsupported feature type {feat_type}.&#34;
            raise ValueError(err_msg)

        gltr_bins[feat_type] = feat_bins

    # Compute features
    lm = LM()

    for j, df in enumerate(dfs):

        dfs[j] = df.progress_apply(
            lambda x: lm.probabilities_feat(x, gltr_bins, topk=5),
            axis=1
        )

    train_df = dfs[0]
    if test_df is not None:
        test_df = dfs[1]

    print(f&#34;Number of GLTR features: {train_df.shape[1] - len(init_feat)}.&#34;)

    return train_df, test_df</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.create_idf_feat"><code class="name flex">
<span>def <span class="ident">create_idf_feat</span></span>(<span>seed: int, train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, idf_feat: Optional[List[str]] = None, more_docs: Optional[pandas.core.frame.DataFrame] = None, max_features: int = 10000, delta: float = 0.1, b: float = 0.5, k_1: float = 1.0, svd_components: int = 1, suffix: str = '')</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create tf-idf features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed to use everywhere for reproducibility.</dd>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>idf_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all tf-idf features names to compute. The name of an idf feature is expected to be
of the form "A_B". A is a sequence of characters for the composition performed in
<code><a title="src.preprocessing.features.tfidf.idf_composition" href="tfidf.html#src.preprocessing.features.tfidf.idf_composition">idf_composition()</a></code>. B is the type of feature we want, it
can be "count" for the term frequencies, "lda" for the latent dirichlet allocation, "idf"
for idf features after a PCA transformation that keeps <strong>svd_components</strong> components from
the initial counter features or "cos" for the cosine similarity between the summary and the
original document.</dd>
<dt><strong><code>more_docs</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Additional dataframe containing a "document" column to use for idf computation.</dd>
<dt><strong><code>max_features</code></strong> :&ensp;<code>int</code>, default=<code>10000</code></dt>
<dd>Maximum number of features in the counter.</dd>
<dt><strong><code>delta</code></strong> :&ensp;<code>float</code>, default=<code>0.1</code></dt>
<dd>Parameter value for "d" composition in tf-idf.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Parameter value for "p" composition in tf-idf.</dd>
<dt><strong><code>k_1</code></strong> :&ensp;<code>float</code>, default=<code>1.0</code></dt>
<dd>Parameter value for "k" composition in tf-idf.</dd>
<dt><strong><code>svd_components</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>Number of components for the truncated SVD component analysis. This number must be smaller
than the number of features in the counter.</dd>
<dt><strong><code>suffix</code></strong> :&ensp;<code>str</code>, default=<code>""</code></dt>
<dd>Suffix for the summary and document columns for specific features such as counter on tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_idf_feat(
    seed: int, train_df: DataFrame, test_df: Optional[DataFrame] = None,
    idf_feat: Optional[List[str]] = None, more_docs: Optional[DataFrame] = None,
    max_features: int = 10000, delta: float = 0.1, b: float = 0.5, k_1: float = 1.,
    svd_components: int = 1, suffix: str = &#34;&#34;
):
    &#34;&#34;&#34;Auxiliary function to create tf-idf features.

    Parameters
    ----------
    seed : int
        Seed to use everywhere for reproducibility.

    train_df : DataFrame
        Training dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    test_df : optional DataFrame, default=None
        Test dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    idf_feat : optional list of str, default=None
        List of all tf-idf features names to compute. The name of an idf feature is expected to be
        of the form &#34;A_B&#34;. A is a sequence of characters for the composition performed in
        `src.preprocessing.features.tfidf.idf_composition()`. B is the type of feature we want, it
        can be &#34;count&#34; for the term frequencies, &#34;lda&#34; for the latent dirichlet allocation, &#34;idf&#34;
        for idf features after a PCA transformation that keeps **svd_components** components from
        the initial counter features or &#34;cos&#34; for the cosine similarity between the summary and the
        original document.

    more_docs : optional DataFrame, default=None
        Additional dataframe containing a &#34;document&#34; column to use for idf computation.

    max_features : int, default=10000
        Maximum number of features in the counter.

    delta : float, default=0.1
        Parameter value for &#34;d&#34; composition in tf-idf.

    b : float, default=0.5
        Parameter value for &#34;p&#34; composition in tf-idf.

    k_1 : float, default=1.0
        Parameter value for &#34;k&#34; composition in tf-idf.

    svd_components : int, default=1
        Number of components for the truncated SVD component analysis. This number must be smaller
        than the number of features in the counter.

    suffix : str, default=&#34;&#34;
        Suffix for the summary and document columns for specific features such as counter on tags.
    &#34;&#34;&#34;
    if idf_feat is None or len(idf_feat) == 0:
        return

    sum_col = &#34;summary&#34; + suffix
    doc_col = &#34;document&#34; + suffix

    print(&#34;\nComputing idf features...&#34;)

    init_feat = set(train_df.columns)
    new_feat = set()

    if more_docs is not None:
        all_docs = pd.concat((train_df[doc_col], more_docs[doc_col]))
    else:
        all_docs = train_df[doc_col]

    # Step 1: Count term frequencies on all documents and summaries
    ctr = CountVectorizer(max_features=max_features)
    ctr.fit(all_docs)  # train only on human-written documents
    train_d_cnt = ctr.transform(all_docs)
    train_s_cnt = ctr.transform(train_df[sum_col])
    if test_df is not None:
        test_d_cnt = ctr.transform(test_df[doc_col])
        test_s_cnt = ctr.transform(test_df[sum_col])

    # Step 2: Compute the inverse document frequency and extract the diagonal matrix
    t = TfidfTransformer(smooth_idf=False)
    t.fit(train_d_cnt)
    idf = t._idf_diag  # pylint: disable=protected-access

    # Step 3: Compute the different features
    counts = [train_d_cnt, train_s_cnt]
    if test_df is not None:
        counts.extend([test_d_cnt, test_s_cnt])

    d = len(all_docs)
    avdl = all_docs.str.len().mean()

    for feat_name in idf_feat:

        feat_split = feat_name.split(&#34;_&#34;)
        feat_prefix = feat_split[0]
        feat_type = feat_split[-1]

        # Step 3.1: Compose the term frequency matrices
        copies = idf_composition(feat_prefix, counts, d, avdl, delta=delta, b=b, k_1=k_1)

        # Step 3.2: Compute the tf-idf values
        copies_idf = [mtrx * idf for mtrx in copies]

        if feat_type == &#34;count&#34;:

            svd = TruncatedSVD(n_components=svd_components, random_state=seed)
            svd.fit(copies[0])

            svd_col = [feat_name + suffix + f&#34;_PCA_{i}&#34; for i in range(1, svd_components + 1)]

            train_df[svd_col] = pd.DataFrame(svd.transform(copies[1]), index=train_df.index)
            if test_df is not None:
                test_df[svd_col] = pd.DataFrame(svd.transform(copies[3]), index=test_df.index)

            new_feat = new_feat.union(svd_col)

        elif feat_type == &#34;lda&#34;:

            lda = LatentDirichletAllocation(n_components=3, random_state=seed, n_jobs=-1)

            lda.fit(copies[0])

            lda_col = [feat_name + suffix + f&#34;_{i}&#34; for i in range(3)]

            train_df[lda_col] = pd.DataFrame(lda.transform(copies[1]), index=train_df.index)
            if test_df is not None:
                test_df[lda_col] = pd.DataFrame(lda.transform(copies[3]), index=test_df.index)

            new_feat = new_feat.union(lda_col)

        elif feat_type == &#34;idf&#34;:

            svd = TruncatedSVD(n_components=svd_components, random_state=seed)
            svd.fit(copies_idf[0])

            svd_col = [feat_name + suffix + f&#34;_PCA_{i}&#34; for i in range(1, svd_components + 1)]

            train_df[svd_col] = pd.DataFrame(svd.transform(copies_idf[1]), index=train_df.index)
            if test_df is not None:
                test_df[svd_col] = pd.DataFrame(svd.transform(copies_idf[3]), index=test_df.index)

            new_feat = new_feat.union(svd_col)

        elif feat_type == &#34;cos&#34;:

            train_df[feat_name + suffix] = [
                cosine_similarity(copies_idf[0][i:i+1], copies_idf[1][i:i+1]).item()
                for i in range(len(train_df))
            ]
            if test_df is not None:
                test_df[feat_name + suffix] = [
                    cosine_similarity(copies_idf[2][i:i+1], copies_idf[3][i:i+1]).item()
                    for i in range(len(test_df))
                ]

            new_feat.add(feat_name + suffix)

        else:

            err_msg = f&#34;From idf feature {feat_name}, unsupported feature type {feat_type}.&#34;
            raise ValueError(err_msg)

    # Drop intermediary features
    inter_feat = set(train_df.columns).difference(init_feat.union(new_feat))

    train_df.drop(columns=inter_feat, inplace=True)
    if test_df is not None:
        test_df.drop(columns=inter_feat, inplace=True)

    print(f&#34;Number of idf features: {len(new_feat)}.&#34;)</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.create_poly_feat"><code class="name flex">
<span>def <span class="ident">create_poly_feat</span></span>(<span>train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, fixed_poly_feat: Optional[List[str]] = None, poly_feat: Optional[List[str]] = None, poly_degree: int = 2)</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create polynomial features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing the initial features.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing the initial features.</dd>
<dt><strong><code>fixed_poly_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of specific polynomial features. A fixed polynomial feature must be of the form
"A B C". A, B and C can be features or powers of features, and their product will be
computed.</dd>
<dt><strong><code>poly_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of polynomial features to compute of which interaction terms will be computed.</dd>
<dt><strong><code>poly_degree</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>Define the degree until which products and powers of features are computed. If 1 or less,
there will be no polynomial features.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_poly_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    fixed_poly_feat: Optional[List[str]] = None, poly_feat: Optional[List[str]] = None,
    poly_degree: int = 2
):
    &#34;&#34;&#34;Auxiliary function to create polynomial features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing the initial features.

    test_df : optional DataFrame, default=None
        Test dataframe containing the initial features.

    fixed_poly_feat : optional list of str, default=None
        List of specific polynomial features. A fixed polynomial feature must be of the form
        &#34;A B C&#34;. A, B and C can be features or powers of features, and their product will be
        computed.

    poly_feat : optional list of str, default=None
        List of polynomial features to compute of which interaction terms will be computed.

    poly_degree : int, default=2
        Define the degree until which products and powers of features are computed. If 1 or less,
        there will be no polynomial features.
    &#34;&#34;&#34;
    if (
        (fixed_poly_feat is None or len(fixed_poly_feat) == 0) and
        (poly_feat is None or len(poly_feat) == 0)
    ):
        return

    print(&#34;\nComputing polynomial features...&#34;)

    init_feat = set(train_df.columns)
    new_feat = set()

    # Work on fixed polynomial features
    if fixed_poly_feat is not None:

        for feat_name in fixed_poly_feat:

            feat_split = feat_name.split(&#34; &#34;)

            for subfeat_name in feat_split:

                apply_poly(train_df, subfeat_name)
                if test_df is not None:
                    apply_poly(test_df, subfeat_name)

            train_df[feat_name] = train_df[feat_split].product(axis=1)
            if test_df is not None:
                test_df[feat_name] = test_df[feat_split].product(axis=1)

            new_feat.add(feat_name)

    # Work on interaction features
    if poly_degree &gt;= 2 and poly_feat is not None and len(poly_feat) &gt; 0:

        # Fit the handler and find the names of the new features
        pf = PolynomialFeatures(degree=(2, poly_degree), include_bias=False)

        pf.fit(train_df[poly_feat])

        new_poly_feat = pf.get_feature_names_out(poly_feat)

        # Actually compute these new features
        train_df[new_poly_feat] = pf.fit_transform(train_df[poly_feat])
        if test_df is not None:
            test_df[new_poly_feat] = pf.transform(test_df[poly_feat])

        new_feat = new_feat.union(set(new_poly_feat))

    # Drop intermediary features
    inter_feat = set(train_df.columns).difference(init_feat.union(new_feat))

    train_df.drop(columns=inter_feat, inplace=True)
    if test_df is not None:
        test_df.drop(columns=inter_feat, inplace=True)

    print(f&#34;Number of tagging features: {len(new_feat)}.&#34;)</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.create_regex_feat"><code class="name flex">
<span>def <span class="ident">create_regex_feat</span></span>(<span>train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, regex_feat: Optional[List[str]] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create regex features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>regex_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all regex features to compute. The name of a regex feature should be of the form
"A_B". A is the name of the regex expression, for example "upper_word". B is the type of
feature we want, it can be "count" for the number of instances, "avg" for the average
length of instances, "overlap" for the number of instances both found in the summary and
the original document or "ratio" for the ratio between the number of instances found in the
summary and the number of instances found in the document.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If one of the features is not supported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_regex_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    regex_feat: Optional[List[str]] = None
):
    &#34;&#34;&#34;Auxiliary function to create regex features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    test_df : optional DataFrame, default=None
        Test dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    regex_feat : optional list of str, default=None
        List of all regex features to compute. The name of a regex feature should be of the form
        &#34;A_B&#34;. A is the name of the regex expression, for example &#34;upper_word&#34;. B is the type of
        feature we want, it can be &#34;count&#34; for the number of instances, &#34;avg&#34; for the average
        length of instances, &#34;overlap&#34; for the number of instances both found in the summary and
        the original document or &#34;ratio&#34; for the ratio between the number of instances found in the
        summary and the number of instances found in the document.

    Raises
    ------
    ValueError
        If one of the features is not supported.
    &#34;&#34;&#34;
    if regex_feat is None or len(regex_feat) == 0:
        return

    print(&#34;\nComputing regex features...&#34;)

    init_feat = set(train_df.columns)
    new_feat = set()

    dfs = [train_df]
    if test_df is not None:
        dfs.append(test_df)

    for feat_name in regex_feat:

        for df in dfs:

            feat_split = feat_name.split(&#34;_&#34;)
            feat_type = feat_split[-1]
            feat_regex_name = &#34;_&#34;.join(feat_split[:-1])
            feat_regex_expr = regex_name_to_expr(feat_regex_name)

            if feat_type == &#34;count&#34;:

                apply_regex_rule(&#34;count&#34;, df, feat_regex_expr, feat_name)

            elif feat_type == &#34;avg&#34;:

                apply_regex_rule(&#34;avg&#34;, df, feat_regex_expr, feat_name)

            elif feat_type == &#34;overlap&#34;:

                s_feat_name = feat_regex_name + &#34;_set&#34;
                d_feat_name = &#34;d_&#34; + feat_regex_name + &#34;_set&#34;

                apply_regex_rule(&#34;set&#34;, df, feat_regex_expr, s_feat_name)
                apply_regex_rule(&#34;set&#34;, df, feat_regex_expr, d_feat_name, summary=False)
                df[feat_name] = [
                    len(x[0] &amp; x[1]) for x in df[[s_feat_name, d_feat_name]].values
                ]

            elif feat_type == &#34;ratio&#34;:

                s_feat_name = feat_regex_name + &#34;_count&#34;
                d_feat_name = &#34;d_&#34; + feat_regex_name + &#34;_count&#34;

                apply_regex_rule(&#34;count&#34;, df, feat_regex_expr, s_feat_name)
                apply_regex_rule(&#34;count&#34;, df, feat_regex_expr, d_feat_name, summary=False)
                df[feat_name] = df[s_feat_name] / df[d_feat_name]

            else:

                err_msg = f&#34;From regex feature {feat_name}, unsupported feature type {feat_type}.&#34;
                raise ValueError(err_msg)

        new_feat.add(feat_name)

    # Drop intermediary features
    inter_feat = set(train_df.columns).difference(init_feat.union(new_feat))

    train_df.drop(columns=inter_feat, inplace=True)
    if test_df is not None:
        test_df.drop(columns=inter_feat, inplace=True)

    print(f&#34;Number of regex features: {len(new_feat)}.&#34;)</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.create_tagging_feat"><code class="name flex">
<span>def <span class="ident">create_tagging_feat</span></span>(<span>seed: int, train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, tag_feat: Optional[List[str]] = None, more_docs: Optional[pandas.core.frame.DataFrame] = None, delta: float = 0.1, b: float = 0.5, k_1: float = 1.0, svd_components: int = 1)</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create tagging features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>Seed to use everywhere for reproducibility.</dd>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing "summary" and "document" columns.</dd>
<dt><strong><code>tag_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all tagging features to compute. The name of a tagging feature must be of the form
"A_B". A is a tag from all available tags in <code>nltk</code> or "tags". B is the type of feature we
want, it can be "count" for the number of instances, "avg" for the average length of
instances, "overlap" for the number of instances both found in the summary and the original
document, "ratio" for the ratio between the number of instances found in the summary and
the number of instances found in the document or "C_D" where C and D are parameters for idf
features.</dd>
<dt><strong><code>more_docs</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Additional dataframe containing a "document" column to use for idf computation.</dd>
<dt><strong><code>delta</code></strong> :&ensp;<code>float</code>, default=<code>0.1</code></dt>
<dd>Parameter value for "d" composition in tf-idf.</dd>
<dt><strong><code>b</code></strong> :&ensp;<code>float</code>, default=<code>0.5</code></dt>
<dd>Parameter value for "p" composition in tf-idf.</dd>
<dt><strong><code>k_1</code></strong> :&ensp;<code>float</code>, default=<code>1.0</code></dt>
<dd>Parameter value for "k" composition in tf-idf.</dd>
<dt><strong><code>svd_components</code></strong> :&ensp;<code>int</code>, default=<code>1</code></dt>
<dd>Number of components for the truncated SVD component analysis. This number must be smaller
than the number of features in the counter.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If one of the features is not supported. If one of the tags is not in the available
nltk tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_tagging_feat(
    seed: int, train_df: DataFrame, test_df: Optional[DataFrame] = None,
    tag_feat: Optional[List[str]] = None, more_docs: Optional[DataFrame] = None,
    delta: float = 0.1, b: float = 0.5, k_1: float = 1., svd_components: int = 1
):
    &#34;&#34;&#34;Auxiliary function to create tagging features.

    Parameters
    ----------
    seed : int
        Seed to use everywhere for reproducibility.

    train_df : DataFrame
        Training dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    test_df : optional DataFrame, default=None
        Test dataframe containing &#34;summary&#34; and &#34;document&#34; columns.

    tag_feat : optional list of str, default=None
        List of all tagging features to compute. The name of a tagging feature must be of the form
        &#34;A_B&#34;. A is a tag from all available tags in `nltk` or &#34;tags&#34;. B is the type of feature we
        want, it can be &#34;count&#34; for the number of instances, &#34;avg&#34; for the average length of
        instances, &#34;overlap&#34; for the number of instances both found in the summary and the original
        document, &#34;ratio&#34; for the ratio between the number of instances found in the summary and
        the number of instances found in the document or &#34;C_D&#34; where C and D are parameters for idf
        features.

    more_docs : optional DataFrame, default=None
        Additional dataframe containing a &#34;document&#34; column to use for idf computation.

    delta : float, default=0.1
        Parameter value for &#34;d&#34; composition in tf-idf.

    b : float, default=0.5
        Parameter value for &#34;p&#34; composition in tf-idf.

    k_1 : float, default=1.0
        Parameter value for &#34;k&#34; composition in tf-idf.

    svd_components : int, default=1
        Number of components for the truncated SVD component analysis. This number must be smaller
        than the number of features in the counter.

    Raises
    ------
    ValueError
        If one of the features is not supported. If one of the tags is not in the available
        nltk tags.
    &#34;&#34;&#34;
    if tag_feat is None or len(tag_feat) == 0:
        return

    print(&#34;\nComputing tagging features...&#34;)

    init_feat = set(train_df.columns)
    new_feat = set()

    dfs = [train_df]
    if test_df is not None:
        dfs.append(test_df)

    stopwords = stopwords_collection.words(&#34;english&#34;)
    punct = string.punctuation.replace(&#34;-&#34;, &#34;&#34;)
    vip_tag = &#34;&#34;

    for feat_name in tag_feat:

        # Extract feature options
        feat_split = feat_name.split(&#34;_&#34;)
        vip_tag = feat_split[0]
        feat_type = &#34;_&#34;.join(feat_split[1:])

        if vip_tag not in ALL_NLTK_TAGS + [&#34;tags&#34;]:

            err_msg = f&#34;Unsupported nltk tag {vip_tag}.&#34;
            raise ValueError(err_msg)

        # Compute tags of summaries and eventually of documents
        for df in dfs:

            if &#34;summary_tagged&#34; not in df.columns:

                df[&#34;summary_tagged&#34;] = df[&#34;summary&#34;].apply(
                    lambda x: text_to_tagged_tokens(
                        x, stopwords, punct, remove_stopwords=False, stemming=False,
                        pos_filtering=False,
                    )
                )

            if (
                (feat_type in {&#34;ratio&#34;, &#34;overlap&#34;} or vip_tag == &#34;tags&#34;) and
                &#34;document_tagged&#34; not in df.columns
            ):

                df[&#34;document_tagged&#34;] = df[&#34;document&#34;].apply(
                    lambda x: text_to_tagged_tokens(
                        x, stopwords, punct, remove_stopwords=False, stemming=False,
                        pos_filtering=False
                    )
                )

        if (
            vip_tag == &#34;tags&#34; and
            (more_docs is not None and &#34;document_tagged&#34; not in more_docs.columns)
        ):

            more_docs[&#34;document_tagged&#34;] = more_docs[&#34;document&#34;].apply(
                lambda x: text_to_tagged_tokens(
                    x, stopwords, punct, remove_stopwords=False, stemming=False,
                    pos_filtering=False
                )
            )

        # Compute tagging feature
        if vip_tag == &#34;tags&#34;:

            suffix = &#34;_&#34; + vip_tag
            max_features = 100
            svd_components = 25

            train_df[&#34;summary_tags&#34;] = train_df[&#34;summary_tagged&#34;].apply(extract_tags)
            train_df[&#34;document_tags&#34;] = train_df[&#34;document_tagged&#34;].apply(extract_tags)
            if test_df is not None:
                test_df[&#34;summary_tags&#34;] = test_df[&#34;summary_tagged&#34;].apply(extract_tags)
                test_df[&#34;document_tags&#34;] = test_df[&#34;document_tagged&#34;].apply(extract_tags)
            if more_docs is not None:
                more_docs[&#34;document_tags&#34;] = more_docs[&#34;document_tagged&#34;].apply(extract_tags)

            create_idf_feat(
                seed, train_df, test_df=test_df, more_docs=more_docs, idf_feat=[feat_type],
                max_features=max_features, delta=delta, b=b, k_1=k_1,
                svd_components=svd_components, suffix=suffix
            )

            if feat_type == &#34;count&#34;:
                new_cols = [feat_name + suffix + f&#34;_PCA_{i}&#34; for i in range(1, svd_components + 1)]
            elif feat_type == &#34;lda&#34;:
                new_cols = [feat_name + suffix + f&#34;_{i}&#34; for i in range(3)]
            elif feat_type == &#34;idf&#34;:
                new_cols = [feat_name + suffix + f&#34;_PCA_{i}&#34; for i in range(1, svd_components + 1)]
            elif feat_type == &#34;cos&#34;:
                new_cols = [feat_name + suffix]
            else:
                new_cols = []

            new_feat.union(set(new_cols))

        else:

            for df in dfs:

                if feat_type == &#34;count&#34;:

                    apply_tag_rule(&#34;count&#34;, df, vip_tag, feat_name)

                elif feat_type == &#34;avg&#34;:

                    apply_tag_rule(&#34;avg&#34;, df, vip_tag, feat_name)

                elif feat_type == &#34;overlap&#34;:

                    s_feat_name = vip_tag + &#34;_set&#34;
                    d_feat_name = &#34;d_&#34; + vip_tag + &#34;_set&#34;

                    apply_tag_rule(&#34;set&#34;, df, vip_tag, s_feat_name)
                    apply_tag_rule(&#34;set&#34;, df, vip_tag, d_feat_name, summary=False)
                    df[feat_name] = [
                        len(x[0] &amp; x[1]) for x in df[[s_feat_name, d_feat_name]].values
                    ]

                elif feat_type == &#34;ratio&#34;:

                    s_feat_name = vip_tag + &#34;_count&#34;
                    d_feat_name = &#34;d_&#34; + vip_tag + &#34;_count&#34;

                    apply_tag_rule(&#34;count&#34;, df, vip_tag, s_feat_name)
                    apply_tag_rule(&#34;count&#34;, df, vip_tag, d_feat_name, summary=False)
                    df[feat_name] = df[s_feat_name] / df[d_feat_name]

                else:

                    err_msg = f&#34;From tagging feature {feat_name},&#34;
                    err_msg += f&#34; unsupported feature type {feat_type}.&#34;
                    raise ValueError(err_msg)

            new_feat.add(feat_name)

    # Drop intermediary features
    inter_feat = set(train_df.columns).difference(init_feat.union(new_feat))

    train_df.drop(columns=inter_feat, inplace=True)
    if test_df is not None:
        test_df.drop(columns=inter_feat, inplace=True)

    print(f&#34;Number of tagging features: {len(new_feat)}.&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.preprocessing" href="../index.html">src.preprocessing</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="src.preprocessing.features.embeddings" href="embeddings.html">src.preprocessing.features.embeddings</a></code></li>
<li><code><a title="src.preprocessing.features.gltr" href="gltr.html">src.preprocessing.features.gltr</a></code></li>
<li><code><a title="src.preprocessing.features.manual_regex" href="manual_regex.html">src.preprocessing.features.manual_regex</a></code></li>
<li><code><a title="src.preprocessing.features.polynomial" href="polynomial.html">src.preprocessing.features.polynomial</a></code></li>
<li><code><a title="src.preprocessing.features.pos_tagging" href="pos_tagging.html">src.preprocessing.features.pos_tagging</a></code></li>
<li><code><a title="src.preprocessing.features.tfidf" href="tfidf.html">src.preprocessing.features.tfidf</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="src.preprocessing.features.create_embed_feat" href="#src.preprocessing.features.create_embed_feat">create_embed_feat</a></code></li>
<li><code><a title="src.preprocessing.features.create_gltr_feat" href="#src.preprocessing.features.create_gltr_feat">create_gltr_feat</a></code></li>
<li><code><a title="src.preprocessing.features.create_idf_feat" href="#src.preprocessing.features.create_idf_feat">create_idf_feat</a></code></li>
<li><code><a title="src.preprocessing.features.create_poly_feat" href="#src.preprocessing.features.create_poly_feat">create_poly_feat</a></code></li>
<li><code><a title="src.preprocessing.features.create_regex_feat" href="#src.preprocessing.features.create_regex_feat">create_regex_feat</a></code></li>
<li><code><a title="src.preprocessing.features.create_tagging_feat" href="#src.preprocessing.features.create_tagging_feat">create_tagging_feat</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>