<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.preprocessing.features.gltr API documentation</title>
<meta name="description" content="Statistical detection of human-written or GPT-2/BERT generated texts …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.preprocessing.features.gltr</code></h1>
</header>
<section id="section-intro">
<p>Statistical detection of human-written or GPT-2/BERT generated texts.</p>
<p>Code adapted from <a href="https://github.com/HendrikStrobelt/detecting-fake-text.">https://github.com/HendrikStrobelt/detecting-fake-text.</a></p>
<h2 id="references">References</h2>
<p>.. [1] Sebastian Gehrmann, Hendrik Strobelt and Alexander M. Rush. <em>GLTR: Statistical Detection and
Visualization of Generated Text.</em> June 2019. (Available at: <a href="https://arxiv.org/abs/1906.04043">https://arxiv.org/abs/1906.04043</a>)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Statistical detection of human-written or GPT-2/BERT generated texts.

Code adapted from https://github.com/HendrikStrobelt/detecting-fake-text.

References
----------
.. [1] Sebastian Gehrmann, Hendrik Strobelt and Alexander M. Rush. *GLTR: Statistical Detection and
    Visualization of Generated Text.* June 2019. (Available at: https://arxiv.org/abs/1906.04043)
&#34;&#34;&#34;


from typing import Any, Dict, List, Optional, Tuple

from tqdm import tqdm

import numpy as np
import pandas as pd
from pandas import DataFrame, Series

import torch
from torch import Tensor

from transformers import GPT2LMHeadModel, GPT2Tokenizer


tqdm.pandas()


class AbstractLanguageChecker:
    &#34;&#34;&#34;Abstract Class that defines the Backend API of GLTR.

    To extend the GLTR interface, you need to inherit this and fill in the defined functions.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Load all necessary components for the other functions.

        Typically, this will comprise a tokenizer and a model.
        &#34;&#34;&#34;
        self.device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

    def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Function that GLTR interacts with to check the probabilities of words.

        Parameters
        ----------
        in_text : str
            The text that you want to check.

        topk : int, default=40
            Your desired truncation of the head of the distribution.

        Returns
        -------
        payload : dict
            The wrapper for results in this function, described below.

        Notes
        -----
            Values in the **payload** returned dict:
            - bpe_strings: list of str -- Each individual token in the text
            - real_topk: list of tuples -- (ranking, prob) of each token
            - pred_topk: list of list of tuple -- (word, prob) for all topk
        &#34;&#34;&#34;
        raise NotImplementedError

    def probabilities_feat(self, df: Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
        &#34;&#34;&#34;Function that GLTR interacts with to add topk features to a data set.

        Parameters
        ----------
        df : Series
            Initial data set with a &#34;summary&#34; column in it.

        bins : dict of {str: int}
            Dictionary which associates a feature to its number of bins.

        topk : int, default=40
            Your desired truncation of the head of the distribution.

        Returns
        -------
        df : Series
            Transformed data set with appended topk features.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def postprocess(self, token: str) -&gt; str:
        &#34;&#34;&#34;Clean up the tokens from any special chars and encode.

        The leading space is encoded by UTF-8 code &#34;\u0120&#34;, linebreak with UTF-8 code 266
        &#34;\u010A&#34;.

        Parameters
        ----------
        token : str
            Raw token text.

        Returns
        -------
        str
            Cleaned and re-encoded token text.
        &#34;&#34;&#34;
        raise NotImplementedError


class LM(AbstractLanguageChecker):
    &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;

    def __init__(self, model_name_or_path: str = &#34;gpt2&#34;):
        &#34;&#34;&#34;Initialize a GPT-2 language checker.

        Parameters
        ----------
        model_name_or_path : str, default=&#34;gpt2&#34;
            Name of the model or path to the pretrained file.
        &#34;&#34;&#34;
        super().__init__()

        self.enc = GPT2Tokenizer.from_pretrained(
            model_name_or_path, cache_dir=&#34;data/transformers_cache&#34;
        )

        self.model = GPT2LMHeadModel.from_pretrained(
            model_name_or_path, cache_dir=&#34;data/transformers_cache&#34;
        )
        self.model.to(self.device)
        self.model.eval()

        self.start_token = self.enc(self.enc.bos_token, return_tensors=&#34;pt&#34;).data[&#34;input_ids&#34;][0]

        print(&#34;Loaded GPT-2 model!&#34;)

    def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        # Process input
        token_ids = self.enc(in_text, return_tensors=&#34;pt&#34;).data[&#34;input_ids&#34;][0]
        token_ids = torch.concat([self.start_token, token_ids])

        # Forward through the model
        output = self.model(token_ids.to(self.device))
        all_logits = output.logits[:-1].detach().squeeze()

        # Construct target and pred
        # yhat = torch.softmax(logits[0, :-1], dim=-1)
        all_probs = torch.softmax(all_logits, dim=1)
        y = token_ids[1:]

        # Sort the predictions for each timestep
        sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()

        # Format [(pos, prob), ...]
        real_topk_pos = [
            int(np.where(sorted_preds[i] == y[i].item())[0][0]) for i in range(y.shape[0])
        ]
        real_topk_probs = all_probs[np.arange(0, y.shape[0], 1), y].data.cpu().numpy().tolist()
        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))

        real_topk = list(zip(real_topk_pos, real_topk_probs))

        # Format [str, str, ...]
        bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])
        bpe_strings = [self.postprocess(s) for s in bpe_strings]  # pylint: disable=not-an-iterable

        topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)

        pred_topk = [
            list(zip(
                self.enc.convert_ids_to_tokens(topk_prob_inds[i]),
                topk_prob_values[i].data.cpu().numpy().tolist()
            )) for i in range(y.shape[0])
        ]
        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]

        payload = {
            &#34;bpe_strings&#34;: bpe_strings,
            &#34;real_topk&#34;: real_topk,
            &#34;pred_topk&#34;: pred_topk
        }

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return payload

    def probabilities_feat(self, df: pd.Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        # Process input
        in_text = df[&#34;summary&#34;]

        # Compute the features
        payload = self.check_probabilities(in_text, topk=topk)
        real_topk = payload[&#34;real_topk&#34;]

        eps = 1e-7

        if &#34;count&#34; in nb_bins:

            topk_count = np.array([real_topk[i][0] for i in range(len(real_topk))])
            count_bins = np.linspace(
                np.log(min(topk_count) + eps) - eps, np.log(max(topk_count) + eps) + eps,
                num=nb_bins[&#34;count&#34;] + 1
            )
            count_hist, _ = np.histogram(topk_count, bins=count_bins)

            count_feat_names = [f&#34;topk_count_{i}&#34; for i in range(len(count_bins) - 1)]

            for count_name, count_feat in zip(count_feat_names, count_hist):
                df[count_name] = count_feat

        if &#34;frac&#34; in nb_bins:

            topk_frac = np.array([real_topk[i][1] for i in range(len(real_topk))])
            frac_bins = np.linspace(
                min(topk_frac) - eps, max(topk_frac) + eps, num=nb_bins[&#34;frac&#34;] + 1
            )
            frac_hist, _ = np.histogram(topk_frac, bins=frac_bins)

            frac_feat_names = [f&#34;topk_frac_{i}&#34; for i in range(len(frac_bins) - 1)]

            for frac_name, frac_feat in zip(frac_feat_names, frac_hist):
                df[frac_name] = frac_feat

        return df

    def sample_unconditional(
        self, length: int = 100, topk: int = 5, temperature: float = 1.
    ) -&gt; str:
        &#34;&#34;&#34;Sample length words from the model.

        Parameters
        ----------
        length : int, default=100
            Number of words to sample.

        topk : int, default=5
            Truncation of the head of the distribution.

        temperature : float, default=1.0
            Temperature for rescaling the predicted logits.

        Returns
        -------
        output_text : str
            Generated words.
        &#34;&#34;&#34;
        context = torch.full(
            (1, 1), self.enc.encoder[self.start_token], device=self.device, dtype=torch.long
        )
        prev = context
        output = context
        past = None

        # Forward through the model
        with torch.no_grad():
            for _ in range(length):

                logits, past = self.model(prev, past=past)
                logits = logits[:, -1, :] / temperature

                # Filter predictions to topk and softmax
                probs = torch.softmax(top_k_logits(logits, k=topk), dim=-1)

                # Sample
                prev = torch.multinomial(probs, num_samples=1)

                # Construct output
                output = torch.cat((output, prev), dim=1)

        output_text = self.enc.decode(output[0].tolist())

        return output_text

    def postprocess(self, token: str) -&gt; str:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        with_space = False
        with_break = False

        if token.startswith(&#34;Ġ&#34;):
            with_space = True
            token = token[1:]

        elif token.startswith(&#34;â&#34;):
            token = &#34; &#34;

        elif token.startswith(&#34;Ċ&#34;):
            token = &#34; &#34;
            with_break = True

        token = &#34;-&#34; if token.startswith(&#34;â&#34;) else token
        token = &#39;“&#39; if token.startswith(&#34;ľ&#34;) else token
        token = &#39;”&#39; if token.startswith(&#34;Ŀ&#34;) else token
        token = &#34;&#39;&#34; if token.startswith(&#34;Ļ&#34;) else token

        if with_space:
            token = &#34;\u0120&#34; + token
        if with_break:
            token = &#34;\u010A&#34; + token

        return token


def top_k_logits(logits: Tensor, k: int) -&gt; Tensor:
    &#34;&#34;&#34;Filter logits to only the top k choices.

    Parameters
    ----------
    logits : Tensor
        Predicted logits.

    Returns
    -------
    topk : Tensor
        Filtered top-k predicted logits with near zero elsewhere.
    &#34;&#34;&#34;
    if k == 0:
        return logits

    values, _ = torch.topk(logits, k)
    min_values = values[:, -1]

    topk = torch.where(
        logits &lt; min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits
    )

    return topk


def create_gltr_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    gltr_feat: Optional[List[str]] = None
) -&gt; Tuple[DataFrame, Optional[DataFrame]]:
    &#34;&#34;&#34;Auxiliary function to create GLTR features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing a &#34;summary&#34; column.

    test_df : optional DataFrame, default=None
        Test dataframe containing a &#34;summary&#34; column.

    gltr_feat : optional list of str, default=None
        List of all GLTR features to compute. The name of a GLTR feature should be of the form
        &#34;A_B&#34;. A is the name of a topk value computed by GLTR, it can be &#34;count&#34; or &#34;frac&#34;. B is
        the number of bins to compute the feature.

    Raises
    ------
    ValueError
        If one of the features is not supported.
    &#34;&#34;&#34;
    if gltr_feat is None or len(gltr_feat) == 0:
        return train_df, test_df

    print(&#34;\nComputing GLTR features...&#34;)

    init_feat = set(train_df.columns)

    dfs = [train_df]
    if test_df is not None:
        dfs.append(test_df)

    # Features parameters
    gltr_bins = {}

    for feat_name in gltr_feat:

        feat_split = feat_name.split(&#34;_&#34;)
        feat_type = feat_split[0]
        feat_bins = int(feat_split[1])

        if feat_type not in {&#34;count&#34;, &#34;frac&#34;}:

            err_msg = f&#34;From GLTR feature {feat_name}, unsupported feature type {feat_type}.&#34;
            raise ValueError(err_msg)

        gltr_bins[feat_type] = feat_bins

    # Compute features
    lm = LM()

    for j, df in enumerate(dfs):

        dfs[j] = df.progress_apply(
            lambda x: lm.probabilities_feat(x, gltr_bins, topk=5),
            axis=1
        )

    train_df = dfs[0]
    if test_df is not None:
        test_df = dfs[1]

    print(f&#34;Number of GLTR features: {train_df.shape[1] - len(init_feat)}.&#34;)

    return train_df, test_df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.preprocessing.features.gltr.create_gltr_feat"><code class="name flex">
<span>def <span class="ident">create_gltr_feat</span></span>(<span>train_df: pandas.core.frame.DataFrame, test_df: Optional[pandas.core.frame.DataFrame] = None, gltr_feat: Optional[List[str]] = None) ‑> Tuple[pandas.core.frame.DataFrame, Optional[pandas.core.frame.DataFrame]]</span>
</code></dt>
<dd>
<div class="desc"><p>Auxiliary function to create GLTR features.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>train_df</code></strong> :&ensp;<code>DataFrame</code></dt>
<dd>Training dataframe containing a "summary" column.</dd>
<dt><strong><code>test_df</code></strong> :&ensp;<code>optional DataFrame</code>, default=<code>None</code></dt>
<dd>Test dataframe containing a "summary" column.</dd>
<dt><strong><code>gltr_feat</code></strong> :&ensp;<code>optional list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of all GLTR features to compute. The name of a GLTR feature should be of the form
"A_B". A is the name of a topk value computed by GLTR, it can be "count" or "frac". B is
the number of bins to compute the feature.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If one of the features is not supported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_gltr_feat(
    train_df: DataFrame, test_df: Optional[DataFrame] = None,
    gltr_feat: Optional[List[str]] = None
) -&gt; Tuple[DataFrame, Optional[DataFrame]]:
    &#34;&#34;&#34;Auxiliary function to create GLTR features.

    Parameters
    ----------
    train_df : DataFrame
        Training dataframe containing a &#34;summary&#34; column.

    test_df : optional DataFrame, default=None
        Test dataframe containing a &#34;summary&#34; column.

    gltr_feat : optional list of str, default=None
        List of all GLTR features to compute. The name of a GLTR feature should be of the form
        &#34;A_B&#34;. A is the name of a topk value computed by GLTR, it can be &#34;count&#34; or &#34;frac&#34;. B is
        the number of bins to compute the feature.

    Raises
    ------
    ValueError
        If one of the features is not supported.
    &#34;&#34;&#34;
    if gltr_feat is None or len(gltr_feat) == 0:
        return train_df, test_df

    print(&#34;\nComputing GLTR features...&#34;)

    init_feat = set(train_df.columns)

    dfs = [train_df]
    if test_df is not None:
        dfs.append(test_df)

    # Features parameters
    gltr_bins = {}

    for feat_name in gltr_feat:

        feat_split = feat_name.split(&#34;_&#34;)
        feat_type = feat_split[0]
        feat_bins = int(feat_split[1])

        if feat_type not in {&#34;count&#34;, &#34;frac&#34;}:

            err_msg = f&#34;From GLTR feature {feat_name}, unsupported feature type {feat_type}.&#34;
            raise ValueError(err_msg)

        gltr_bins[feat_type] = feat_bins

    # Compute features
    lm = LM()

    for j, df in enumerate(dfs):

        dfs[j] = df.progress_apply(
            lambda x: lm.probabilities_feat(x, gltr_bins, topk=5),
            axis=1
        )

    train_df = dfs[0]
    if test_df is not None:
        test_df = dfs[1]

    print(f&#34;Number of GLTR features: {train_df.shape[1] - len(init_feat)}.&#34;)

    return train_df, test_df</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.top_k_logits"><code class="name flex">
<span>def <span class="ident">top_k_logits</span></span>(<span>logits: torch.Tensor, k: int) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Filter logits to only the top k choices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>logits</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Predicted logits.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>topk</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Filtered top-k predicted logits with near zero elsewhere.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def top_k_logits(logits: Tensor, k: int) -&gt; Tensor:
    &#34;&#34;&#34;Filter logits to only the top k choices.

    Parameters
    ----------
    logits : Tensor
        Predicted logits.

    Returns
    -------
    topk : Tensor
        Filtered top-k predicted logits with near zero elsewhere.
    &#34;&#34;&#34;
    if k == 0:
        return logits

    values, _ = torch.topk(logits, k)
    min_values = values[:, -1]

    topk = torch.where(
        logits &lt; min_values, torch.ones_like(logits, dtype=logits.dtype) * -1e10, logits
    )

    return topk</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.preprocessing.features.gltr.AbstractLanguageChecker"><code class="flex name class">
<span>class <span class="ident">AbstractLanguageChecker</span></span>
</code></dt>
<dd>
<div class="desc"><p>Abstract Class that defines the Backend API of GLTR.</p>
<p>To extend the GLTR interface, you need to inherit this and fill in the defined functions.</p>
<p>Load all necessary components for the other functions.</p>
<p>Typically, this will comprise a tokenizer and a model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AbstractLanguageChecker:
    &#34;&#34;&#34;Abstract Class that defines the Backend API of GLTR.

    To extend the GLTR interface, you need to inherit this and fill in the defined functions.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;Load all necessary components for the other functions.

        Typically, this will comprise a tokenizer and a model.
        &#34;&#34;&#34;
        self.device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

    def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;Function that GLTR interacts with to check the probabilities of words.

        Parameters
        ----------
        in_text : str
            The text that you want to check.

        topk : int, default=40
            Your desired truncation of the head of the distribution.

        Returns
        -------
        payload : dict
            The wrapper for results in this function, described below.

        Notes
        -----
            Values in the **payload** returned dict:
            - bpe_strings: list of str -- Each individual token in the text
            - real_topk: list of tuples -- (ranking, prob) of each token
            - pred_topk: list of list of tuple -- (word, prob) for all topk
        &#34;&#34;&#34;
        raise NotImplementedError

    def probabilities_feat(self, df: Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
        &#34;&#34;&#34;Function that GLTR interacts with to add topk features to a data set.

        Parameters
        ----------
        df : Series
            Initial data set with a &#34;summary&#34; column in it.

        bins : dict of {str: int}
            Dictionary which associates a feature to its number of bins.

        topk : int, default=40
            Your desired truncation of the head of the distribution.

        Returns
        -------
        df : Series
            Transformed data set with appended topk features.
        &#34;&#34;&#34;
        raise NotImplementedError()

    def postprocess(self, token: str) -&gt; str:
        &#34;&#34;&#34;Clean up the tokens from any special chars and encode.

        The leading space is encoded by UTF-8 code &#34;\u0120&#34;, linebreak with UTF-8 code 266
        &#34;\u010A&#34;.

        Parameters
        ----------
        token : str
            Raw token text.

        Returns
        -------
        str
            Cleaned and re-encoded token text.
        &#34;&#34;&#34;
        raise NotImplementedError</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.preprocessing.features.gltr.LM" href="#src.preprocessing.features.gltr.LM">LM</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.preprocessing.features.gltr.AbstractLanguageChecker.check_probabilities"><code class="name flex">
<span>def <span class="ident">check_probabilities</span></span>(<span>self, in_text: str, topk: int = 40) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Function that GLTR interacts with to check the probabilities of words.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>in_text</code></strong> :&ensp;<code>str</code></dt>
<dd>The text that you want to check.</dd>
<dt><strong><code>topk</code></strong> :&ensp;<code>int</code>, default=<code>40</code></dt>
<dd>Your desired truncation of the head of the distribution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>payload</code></strong> :&ensp;<code>dict</code></dt>
<dd>The wrapper for results in this function, described below.</dd>
</dl>
<h2 id="notes">Notes</h2>
<pre><code>Values in the **payload** returned dict:
- bpe_strings: list of str -- Each individual token in the text
- real_topk: list of tuples -- (ranking, prob) of each token
- pred_topk: list of list of tuple -- (word, prob) for all topk
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;Function that GLTR interacts with to check the probabilities of words.

    Parameters
    ----------
    in_text : str
        The text that you want to check.

    topk : int, default=40
        Your desired truncation of the head of the distribution.

    Returns
    -------
    payload : dict
        The wrapper for results in this function, described below.

    Notes
    -----
        Values in the **payload** returned dict:
        - bpe_strings: list of str -- Each individual token in the text
        - real_topk: list of tuples -- (ranking, prob) of each token
        - pred_topk: list of list of tuple -- (word, prob) for all topk
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.AbstractLanguageChecker.postprocess"><code class="name flex">
<span>def <span class="ident">postprocess</span></span>(<span>self, token: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Clean up the tokens from any special chars and encode.</p>
<p>The leading space is encoded by UTF-8 code "Ġ", linebreak with UTF-8 code 266
"Ċ".</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>token</code></strong> :&ensp;<code>str</code></dt>
<dd>Raw token text.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>Cleaned and re-encoded token text.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def postprocess(self, token: str) -&gt; str:
    &#34;&#34;&#34;Clean up the tokens from any special chars and encode.

    The leading space is encoded by UTF-8 code &#34;\u0120&#34;, linebreak with UTF-8 code 266
    &#34;\u010A&#34;.

    Parameters
    ----------
    token : str
        Raw token text.

    Returns
    -------
    str
        Cleaned and re-encoded token text.
    &#34;&#34;&#34;
    raise NotImplementedError</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.AbstractLanguageChecker.probabilities_feat"><code class="name flex">
<span>def <span class="ident">probabilities_feat</span></span>(<span>self, df: pandas.core.series.Series, nb_bins: Dict[str, int], topk: int = 40) ‑> pandas.core.series.Series</span>
</code></dt>
<dd>
<div class="desc"><p>Function that GLTR interacts with to add topk features to a data set.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>Series</code></dt>
<dd>Initial data set with a "summary" column in it.</dd>
<dt><strong><code>bins</code></strong> :&ensp;<code>dict</code> of <code>{str: int}</code></dt>
<dd>Dictionary which associates a feature to its number of bins.</dd>
<dt><strong><code>topk</code></strong> :&ensp;<code>int</code>, default=<code>40</code></dt>
<dd>Your desired truncation of the head of the distribution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>Series</code></dt>
<dd>Transformed data set with appended topk features.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def probabilities_feat(self, df: Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
    &#34;&#34;&#34;Function that GLTR interacts with to add topk features to a data set.

    Parameters
    ----------
    df : Series
        Initial data set with a &#34;summary&#34; column in it.

    bins : dict of {str: int}
        Dictionary which associates a feature to its number of bins.

    topk : int, default=40
        Your desired truncation of the head of the distribution.

    Returns
    -------
    df : Series
        Transformed data set with appended topk features.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="src.preprocessing.features.gltr.LM"><code class="flex name class">
<span>class <span class="ident">LM</span></span>
<span>(</span><span>model_name_or_path: str = 'gpt2')</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></code> class for description.</p>
<p>Initialize a GPT-2 language checker.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_name_or_path</code></strong> :&ensp;<code>str</code>, default=<code>"gpt2"</code></dt>
<dd>Name of the model or path to the pretrained file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LM(AbstractLanguageChecker):
    &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;

    def __init__(self, model_name_or_path: str = &#34;gpt2&#34;):
        &#34;&#34;&#34;Initialize a GPT-2 language checker.

        Parameters
        ----------
        model_name_or_path : str, default=&#34;gpt2&#34;
            Name of the model or path to the pretrained file.
        &#34;&#34;&#34;
        super().__init__()

        self.enc = GPT2Tokenizer.from_pretrained(
            model_name_or_path, cache_dir=&#34;data/transformers_cache&#34;
        )

        self.model = GPT2LMHeadModel.from_pretrained(
            model_name_or_path, cache_dir=&#34;data/transformers_cache&#34;
        )
        self.model.to(self.device)
        self.model.eval()

        self.start_token = self.enc(self.enc.bos_token, return_tensors=&#34;pt&#34;).data[&#34;input_ids&#34;][0]

        print(&#34;Loaded GPT-2 model!&#34;)

    def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        # Process input
        token_ids = self.enc(in_text, return_tensors=&#34;pt&#34;).data[&#34;input_ids&#34;][0]
        token_ids = torch.concat([self.start_token, token_ids])

        # Forward through the model
        output = self.model(token_ids.to(self.device))
        all_logits = output.logits[:-1].detach().squeeze()

        # Construct target and pred
        # yhat = torch.softmax(logits[0, :-1], dim=-1)
        all_probs = torch.softmax(all_logits, dim=1)
        y = token_ids[1:]

        # Sort the predictions for each timestep
        sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()

        # Format [(pos, prob), ...]
        real_topk_pos = [
            int(np.where(sorted_preds[i] == y[i].item())[0][0]) for i in range(y.shape[0])
        ]
        real_topk_probs = all_probs[np.arange(0, y.shape[0], 1), y].data.cpu().numpy().tolist()
        real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))

        real_topk = list(zip(real_topk_pos, real_topk_probs))

        # Format [str, str, ...]
        bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])
        bpe_strings = [self.postprocess(s) for s in bpe_strings]  # pylint: disable=not-an-iterable

        topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)

        pred_topk = [
            list(zip(
                self.enc.convert_ids_to_tokens(topk_prob_inds[i]),
                topk_prob_values[i].data.cpu().numpy().tolist()
            )) for i in range(y.shape[0])
        ]
        pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]

        payload = {
            &#34;bpe_strings&#34;: bpe_strings,
            &#34;real_topk&#34;: real_topk,
            &#34;pred_topk&#34;: pred_topk
        }

        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return payload

    def probabilities_feat(self, df: pd.Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        # Process input
        in_text = df[&#34;summary&#34;]

        # Compute the features
        payload = self.check_probabilities(in_text, topk=topk)
        real_topk = payload[&#34;real_topk&#34;]

        eps = 1e-7

        if &#34;count&#34; in nb_bins:

            topk_count = np.array([real_topk[i][0] for i in range(len(real_topk))])
            count_bins = np.linspace(
                np.log(min(topk_count) + eps) - eps, np.log(max(topk_count) + eps) + eps,
                num=nb_bins[&#34;count&#34;] + 1
            )
            count_hist, _ = np.histogram(topk_count, bins=count_bins)

            count_feat_names = [f&#34;topk_count_{i}&#34; for i in range(len(count_bins) - 1)]

            for count_name, count_feat in zip(count_feat_names, count_hist):
                df[count_name] = count_feat

        if &#34;frac&#34; in nb_bins:

            topk_frac = np.array([real_topk[i][1] for i in range(len(real_topk))])
            frac_bins = np.linspace(
                min(topk_frac) - eps, max(topk_frac) + eps, num=nb_bins[&#34;frac&#34;] + 1
            )
            frac_hist, _ = np.histogram(topk_frac, bins=frac_bins)

            frac_feat_names = [f&#34;topk_frac_{i}&#34; for i in range(len(frac_bins) - 1)]

            for frac_name, frac_feat in zip(frac_feat_names, frac_hist):
                df[frac_name] = frac_feat

        return df

    def sample_unconditional(
        self, length: int = 100, topk: int = 5, temperature: float = 1.
    ) -&gt; str:
        &#34;&#34;&#34;Sample length words from the model.

        Parameters
        ----------
        length : int, default=100
            Number of words to sample.

        topk : int, default=5
            Truncation of the head of the distribution.

        temperature : float, default=1.0
            Temperature for rescaling the predicted logits.

        Returns
        -------
        output_text : str
            Generated words.
        &#34;&#34;&#34;
        context = torch.full(
            (1, 1), self.enc.encoder[self.start_token], device=self.device, dtype=torch.long
        )
        prev = context
        output = context
        past = None

        # Forward through the model
        with torch.no_grad():
            for _ in range(length):

                logits, past = self.model(prev, past=past)
                logits = logits[:, -1, :] / temperature

                # Filter predictions to topk and softmax
                probs = torch.softmax(top_k_logits(logits, k=topk), dim=-1)

                # Sample
                prev = torch.multinomial(probs, num_samples=1)

                # Construct output
                output = torch.cat((output, prev), dim=1)

        output_text = self.enc.decode(output[0].tolist())

        return output_text

    def postprocess(self, token: str) -&gt; str:
        &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
        with_space = False
        with_break = False

        if token.startswith(&#34;Ġ&#34;):
            with_space = True
            token = token[1:]

        elif token.startswith(&#34;â&#34;):
            token = &#34; &#34;

        elif token.startswith(&#34;Ċ&#34;):
            token = &#34; &#34;
            with_break = True

        token = &#34;-&#34; if token.startswith(&#34;â&#34;) else token
        token = &#39;“&#39; if token.startswith(&#34;ľ&#34;) else token
        token = &#39;”&#39; if token.startswith(&#34;Ŀ&#34;) else token
        token = &#34;&#39;&#34; if token.startswith(&#34;Ļ&#34;) else token

        if with_space:
            token = &#34;\u0120&#34; + token
        if with_break:
            token = &#34;\u010A&#34; + token

        return token</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.preprocessing.features.gltr.LM.check_probabilities"><code class="name flex">
<span>def <span class="ident">check_probabilities</span></span>(<span>self, in_text: str, topk: int = 40) ‑> Dict[str, Any]</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></code> class for description.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_probabilities(self, in_text: str, topk: int = 40) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
    # Process input
    token_ids = self.enc(in_text, return_tensors=&#34;pt&#34;).data[&#34;input_ids&#34;][0]
    token_ids = torch.concat([self.start_token, token_ids])

    # Forward through the model
    output = self.model(token_ids.to(self.device))
    all_logits = output.logits[:-1].detach().squeeze()

    # Construct target and pred
    # yhat = torch.softmax(logits[0, :-1], dim=-1)
    all_probs = torch.softmax(all_logits, dim=1)
    y = token_ids[1:]

    # Sort the predictions for each timestep
    sorted_preds = torch.argsort(all_probs, dim=1, descending=True).cpu()

    # Format [(pos, prob), ...]
    real_topk_pos = [
        int(np.where(sorted_preds[i] == y[i].item())[0][0]) for i in range(y.shape[0])
    ]
    real_topk_probs = all_probs[np.arange(0, y.shape[0], 1), y].data.cpu().numpy().tolist()
    real_topk_probs = list(map(lambda x: round(x, 5), real_topk_probs))

    real_topk = list(zip(real_topk_pos, real_topk_probs))

    # Format [str, str, ...]
    bpe_strings = self.enc.convert_ids_to_tokens(token_ids[:])
    bpe_strings = [self.postprocess(s) for s in bpe_strings]  # pylint: disable=not-an-iterable

    topk_prob_values, topk_prob_inds = torch.topk(all_probs, k=topk, dim=1)

    pred_topk = [
        list(zip(
            self.enc.convert_ids_to_tokens(topk_prob_inds[i]),
            topk_prob_values[i].data.cpu().numpy().tolist()
        )) for i in range(y.shape[0])
    ]
    pred_topk = [[(self.postprocess(t[0]), t[1]) for t in pred] for pred in pred_topk]

    payload = {
        &#34;bpe_strings&#34;: bpe_strings,
        &#34;real_topk&#34;: real_topk,
        &#34;pred_topk&#34;: pred_topk
    }

    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return payload</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.LM.postprocess"><code class="name flex">
<span>def <span class="ident">postprocess</span></span>(<span>self, token: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></code> class for description.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def postprocess(self, token: str) -&gt; str:
    &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
    with_space = False
    with_break = False

    if token.startswith(&#34;Ġ&#34;):
        with_space = True
        token = token[1:]

    elif token.startswith(&#34;â&#34;):
        token = &#34; &#34;

    elif token.startswith(&#34;Ċ&#34;):
        token = &#34; &#34;
        with_break = True

    token = &#34;-&#34; if token.startswith(&#34;â&#34;) else token
    token = &#39;“&#39; if token.startswith(&#34;ľ&#34;) else token
    token = &#39;”&#39; if token.startswith(&#34;Ŀ&#34;) else token
    token = &#34;&#39;&#34; if token.startswith(&#34;Ļ&#34;) else token

    if with_space:
        token = &#34;\u0120&#34; + token
    if with_break:
        token = &#34;\u010A&#34; + token

    return token</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.LM.probabilities_feat"><code class="name flex">
<span>def <span class="ident">probabilities_feat</span></span>(<span>self, df: pandas.core.series.Series, nb_bins: Dict[str, int], topk: int = 40) ‑> pandas.core.series.Series</span>
</code></dt>
<dd>
<div class="desc"><p>See <code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></code> class for description.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def probabilities_feat(self, df: pd.Series, nb_bins: Dict[str, int], topk: int = 40) -&gt; Series:
    &#34;&#34;&#34;See `AbstractLanguageChecker` class for description.&#34;&#34;&#34;
    # Process input
    in_text = df[&#34;summary&#34;]

    # Compute the features
    payload = self.check_probabilities(in_text, topk=topk)
    real_topk = payload[&#34;real_topk&#34;]

    eps = 1e-7

    if &#34;count&#34; in nb_bins:

        topk_count = np.array([real_topk[i][0] for i in range(len(real_topk))])
        count_bins = np.linspace(
            np.log(min(topk_count) + eps) - eps, np.log(max(topk_count) + eps) + eps,
            num=nb_bins[&#34;count&#34;] + 1
        )
        count_hist, _ = np.histogram(topk_count, bins=count_bins)

        count_feat_names = [f&#34;topk_count_{i}&#34; for i in range(len(count_bins) - 1)]

        for count_name, count_feat in zip(count_feat_names, count_hist):
            df[count_name] = count_feat

    if &#34;frac&#34; in nb_bins:

        topk_frac = np.array([real_topk[i][1] for i in range(len(real_topk))])
        frac_bins = np.linspace(
            min(topk_frac) - eps, max(topk_frac) + eps, num=nb_bins[&#34;frac&#34;] + 1
        )
        frac_hist, _ = np.histogram(topk_frac, bins=frac_bins)

        frac_feat_names = [f&#34;topk_frac_{i}&#34; for i in range(len(frac_bins) - 1)]

        for frac_name, frac_feat in zip(frac_feat_names, frac_hist):
            df[frac_name] = frac_feat

    return df</code></pre>
</details>
</dd>
<dt id="src.preprocessing.features.gltr.LM.sample_unconditional"><code class="name flex">
<span>def <span class="ident">sample_unconditional</span></span>(<span>self, length: int = 100, topk: int = 5, temperature: float = 1.0) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Sample length words from the model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>length</code></strong> :&ensp;<code>int</code>, default=<code>100</code></dt>
<dd>Number of words to sample.</dd>
<dt><strong><code>topk</code></strong> :&ensp;<code>int</code>, default=<code>5</code></dt>
<dd>Truncation of the head of the distribution.</dd>
<dt><strong><code>temperature</code></strong> :&ensp;<code>float</code>, default=<code>1.0</code></dt>
<dd>Temperature for rescaling the predicted logits.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output_text</code></strong> :&ensp;<code>str</code></dt>
<dd>Generated words.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_unconditional(
    self, length: int = 100, topk: int = 5, temperature: float = 1.
) -&gt; str:
    &#34;&#34;&#34;Sample length words from the model.

    Parameters
    ----------
    length : int, default=100
        Number of words to sample.

    topk : int, default=5
        Truncation of the head of the distribution.

    temperature : float, default=1.0
        Temperature for rescaling the predicted logits.

    Returns
    -------
    output_text : str
        Generated words.
    &#34;&#34;&#34;
    context = torch.full(
        (1, 1), self.enc.encoder[self.start_token], device=self.device, dtype=torch.long
    )
    prev = context
    output = context
    past = None

    # Forward through the model
    with torch.no_grad():
        for _ in range(length):

            logits, past = self.model(prev, past=past)
            logits = logits[:, -1, :] / temperature

            # Filter predictions to topk and softmax
            probs = torch.softmax(top_k_logits(logits, k=topk), dim=-1)

            # Sample
            prev = torch.multinomial(probs, num_samples=1)

            # Construct output
            output = torch.cat((output, prev), dim=1)

    output_text = self.enc.decode(output[0].tolist())

    return output_text</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#references">References</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.preprocessing.features" href="index.html">src.preprocessing.features</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.preprocessing.features.gltr.create_gltr_feat" href="#src.preprocessing.features.gltr.create_gltr_feat">create_gltr_feat</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.top_k_logits" href="#src.preprocessing.features.gltr.top_k_logits">top_k_logits</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker" href="#src.preprocessing.features.gltr.AbstractLanguageChecker">AbstractLanguageChecker</a></code></h4>
<ul class="">
<li><code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker.check_probabilities" href="#src.preprocessing.features.gltr.AbstractLanguageChecker.check_probabilities">check_probabilities</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker.postprocess" href="#src.preprocessing.features.gltr.AbstractLanguageChecker.postprocess">postprocess</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.AbstractLanguageChecker.probabilities_feat" href="#src.preprocessing.features.gltr.AbstractLanguageChecker.probabilities_feat">probabilities_feat</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.preprocessing.features.gltr.LM" href="#src.preprocessing.features.gltr.LM">LM</a></code></h4>
<ul class="">
<li><code><a title="src.preprocessing.features.gltr.LM.check_probabilities" href="#src.preprocessing.features.gltr.LM.check_probabilities">check_probabilities</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.LM.postprocess" href="#src.preprocessing.features.gltr.LM.postprocess">postprocess</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.LM.probabilities_feat" href="#src.preprocessing.features.gltr.LM.probabilities_feat">probabilities_feat</a></code></li>
<li><code><a title="src.preprocessing.features.gltr.LM.sample_unconditional" href="#src.preprocessing.features.gltr.LM.sample_unconditional">sample_unconditional</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>